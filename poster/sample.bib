@inproceedings{DefinitionGenerationMainArticle,
    title = "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
    author = "Giulianelli, Mario  and
      Luden, Iris  and
      Fernandez, Raquel  and
      Kutuzov, Andrey",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.176/",
    doi = "10.18653/v1/2023.acl-long.176",
    pages = "3130--3148",
    abstract = "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users {---} historical linguists, lexicographers, or social scientists {---} to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the {\textquoteleft}definitions as representations' paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP."
}

@inproceedings{fedorova-etal-2024-definition,
    title = "Definition generation for lexical semantic change detection",
    author = "Fedorova, Mariia  and
      Kutuzov, Andrey  and
      Scherrer, Yves",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.339/",
    doi = "10.18653/v1/2024.findings-acl.339",
    pages = "5712--5724",
    abstract = "We use contextualized word definitions generated by large language models as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as {\textquoteleft}senses', and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sense-based LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling."
}

@article{DefinitionModelingReviewAndDatasetAnalysis,
author = {Gardner, Noah and Khan, Hafiz and Hung, Chih-Cheng},
title = {Definition modeling: literature review and dataset analysis},
journal = {Applied Computing and Intelligence},
pages = {83--98},
volume = {2},
year = {2022}
}

@inproceedings{BLEUScore,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040/",
    doi = "10.3115/1073083.1073135",
    pages = "311--318"
}

@inproceedings{ROUGE,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{BERTScore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{FRED-T5,
    title = "A Family of Pretrained Transformer Language Models for {R}ussian",
    author = "Zmitrovich, Dmitry  and
      Abramov, Aleksandr  and
      Kalmykov, Andrey  and
      Kadulin, Vitaly  and
      Tikhonova, Maria  and
      Taktasheva, Ekaterina  and
      Astafurov, Danil  and
      Baushenko, Mark  and
      Snegirev, Artem  and
      Shavrina, Tatiana  and
      Markov, Sergei S.  and
      Mikhailov, Vladislav  and
      Fenogenova, Alena",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.45/",
    pages = "507--524",
    abstract = "Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language."
}

@inproceedings{RussianSuperGLUE,
year = {2020},
publisher = {Association for Computational Linguistics},
author = {Tatiana Shavrina and Alena Fenogenova and Emelyanov Anton and Denis Shevelev and Ekaterina Artemova and Valentin Malykh and Vladislav Mikhailov and Maria Tikhonova and Andrey Chertok and Andrey Evlampiev},
title = {{RussianSuperGLUE}: A Russian Language Understanding Evaluation Benchmark},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}
}

@misc{Evaluate,
  author       = {{Hugging Face}},
  title        = {Evaluate},
  howpublished = {\url{https://github.com/huggingface/evaluate}},
  year         = {2023},
  note         = {Retrieved November 15, 2023}
}

@misc{Encodechka,
  author       = {Avidale},
  title        = {encodechka},
  howpublished = {\url{https://github.com/avidale/encodechka}},
  year         = {2023},
  note         = {Retrieved April 19, 2024}
}

@misc{Vectorizer,
  author       = {Sentence Transformers},
  title        = {paraphrase-multilingual-mpnet-base-v2},
  howpublished = {\url{https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2}},
  year         = {2023},
  note         = {Retrieved April 19, 2024}
}

@misc{rodina2020elmo,
      title={ELMo and BERT in semantic change detection for Russian},
      author={Julia Rodina and Yuliya Trofimova and Andrey Kutuzov and Ekaterina Artemova},
      year={2020},
}

@inproceedings{ELMOOriginal,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202/",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
}

@inproceedings{BERTOriginal,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{kutuzov-etal-2018-diachronic,
    title = "Diachronic word embeddings and semantic shifts: a survey",
    author = "Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Szymanski, Terrence  and
      Velldal, Erik",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    pages = "1384--1397",
}

@inproceedings{shiftry,
author = {Kutuzov, Andrei and Fomin, V. and Mikhailov, V. and Rodina, Julia},
year = {2020},
month = {01},
booktitle = {Computational Linguistics and Intellectual Technologies},
volume = {19},
pages = {500-516},
title = {ShiftRy: Web service for diachronic analysis of Russian news},
doi = {10.28995/2075-7182-2020-19-500-516}
}

@inproceedings{semeval2020task,
    title = "{S}em{E}val-2020 Task 1: Unsupervised Lexical Semantic Change Detection",
    author = "Schlechtweg, Dominik  and
      McGillivray, Barbara  and
      Hengchen, Simon  and
      Dubossarsky, Haim  and
      Tahmasebi, Nina",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.1/",
    doi = "10.18653/v1/2020.semeval-1.1",
    pages = "1--23"
}

@inproceedings{rushifteval,
  title={RuShiftEval: a shared task on semantic shift detection for Russian},
  author={Kutuzov, Andrey and Pivovarova, Lidia},
  pages = {533-545},
  booktitle={Computational linguistics and intellectual technologies: Papers from the annual conference Dialogue},
  year={2021}
}

@inproceedings{rusemshift,
    title = "{R}u{S}em{S}hift: a dataset of historical lexical semantic change in {R}ussian",
    author = "Rodina, Julia  and
      Kutuzov, Andrey",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.90/",
    doi = "10.18653/v1/2020.coling-main.90",
    pages = "1037--1047",
    abstract = "We present RuSemShift, a large-scale manually annotated test set for the task of semantic change modeling in Russian for two long-term time period pairs: from the pre-Soviet through the Soviet times and from the Soviet through the post-Soviet times. Target words were annotated by multiple crowd-source workers. The annotation process was organized following the DURel framework and was based on sentence contexts extracted from the Russian National Corpus. Additionally, we report the performance of several distributional approaches on RuSemShift, achieving promising results, which at the same time leave room for other researchers to improve."
}

@inproceedings{GlossReader,
  author    = {Rachinskiy, Maxim and Arefyev, Nikolay},
  title     = {Zero-shot Cross-lingual Transfer of a Gloss Language Model for Semantic Change Detection},
  booktitle = {Computational Linguistics and Intellectual Technologies},
  year      = {2021},
  month     = {06},
  volume    = {20},
  pages     = {578--586},
  doi       = {10.28995/2075-7182-2021-20-578-586}
}

@inproceedings{DeepMistake,
  author    = {Arefyev, Nikolay and Fedoseev, Maksim and Protasov, Vitaly and Panchenko, Alexander and Homskiy, Daniil and Davletov, Adis},
  title     = {DeepMistake: Which Senses are Hard to Distinguish for a Word-in-Context Model},
  booktitle = {Computational Linguistics and Intellectual Technologies},
  year      = {2021},
  month     = {06},
  volume    = {20},
  pages     = {16--30},
  doi       = {10.28995/2075-7182-2021-20-16-30}
}


@article{XLM-R,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{DeepMistakeGitHub,
  author       = {Daniil153},
  title        = {DeepMistake},
  howpublished = {\url{https://github.com/Daniil153/DeepMistake}},
  year         = {2023},
  note         = {Retrieved January 18, 2024}
}

@misc{GlossReaderGitHub,
  author       = {Myra Chins},
  title        = {GlossReader},
  howpublished = {\url{https://github.com/myrachins/RuShiftEval}},
  year         = {2023},
  note         = {Retrieved January 18, 2024}
}

@inproceedings{noraset2016definition,
    author = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
    title = {Definition modeling: learning to define word embeddings in natural language},
    year = {2017},
    publisher = {AAAI Press},
    abstract = {Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.},
    booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
    pages = {3259–3266},
    numpages = {8},
    location = {San Francisco, California, USA}, series = {AAAI'17}
}

@inproceedings{gadetsky-etal-2018-conditional,
    title = "Conditional Generators of Words Definitions",
    author = "Gadetsky, Artyom  and
      Yakubovskiy, Ilya  and
      Vetrov, Dmitry",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2043",
    doi = "10.18653/v1/P18-2043",
    pages = "266--271",
    abstract = "We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words{'} ambiguity and polysemy leads to performance improvement.",
}

@inproceedings{huang-etal-2021-definition,
    title = "Definition Modelling for Appropriate Specificity",
    author = "Huang, Han  and
      Kajiwara, Tomoyuki  and
      Arase, Yuki",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.194",
    doi = "10.18653/v1/2021.emnlp-main.194",
    pages = "2499--2509",
    abstract = "Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under-specificity problems. Over-specific definitions present narrow word meanings, whereas under-specific definitions present general and context-insensitive meanings. Herein, we propose a method for definition generation with appropriate specificity. The proposed method addresses the aforementioned problems by leveraging a pre-trained encoder-decoder model, namely Text-to-Text Transfer Transformer, and introducing a re-ranking mechanism to model specificity in definitions. Experimental results on standard evaluation datasets indicate that our method significantly outperforms the previous state-of-the-art method. Moreover, manual evaluation confirms that our method effectively addresses the over/under-specificity problems.",
}

@book{BloomfieldLanguage,
  author    = {Bloomfield, Leonard},
  title     = {Language},
  publisher = {Holt, Rinehart and Winston},
  year      = {1933},
  address   = {New York},
}

@incollection{TahmasebiComputationalApproachesToSemanticChange,
  author    = {Nina Tahmasebi and Lars Borin and Adam Jatowt},
  title     = {Survey of computational approaches to lexical semantic change detection},
  booktitle = {Computational approaches to semantic change},
  editor    = {Nina Tahmasebi and Lars Borin and Adam Jatowt and Yang Xu and Simon Hengchen},
  year      = {2021},
  pages     = {1--91},
  publisher = {Language Science Press},
  address   = {Berlin},
}

@misc{Wiktionary,
  author       = {{Wiktionary contributors}},
  title        = {Wiktionary, the free dictionary},
  howpublished = {\url{https://www.wiktionary.org/}},
  year         = {2023},
  note         = {Retrieved April 10, 2024}
}

@misc{WorkGitHub,
  author       = {Tatarinov, M. D.},
  title        = {Work Definition Modeling},
  howpublished = {\url{https://github.com/tatarinovst2/work-definition-modeling}},
  year         = {2023},
  note         = {Retrieved [Insert Retrieval Date]}
}

@inproceedings{Harris2014SemanticShifts,
  title={Semantic Shift in the English Language},
  author={Timothy M. Harris},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:60071546}
}

@misc{Periti2024AnalyzingLexicalReplacements,
      title={Analyzing Semantic Change through Lexical Replacements},
      author={Francesco Periti and Pierluigi Cassotti and Haim Dubossarsky and Nina Tahmasebi},
      year={2024},
      eprint={2404.18570},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Maysak2016,
  author       = {Майсак, Т. А.},
  title        = {Грамматикализация},
  year         = {2016},
  howpublished = {\emph{Большая российская энциклопедия. Электронная версия}},
  url          = {https://old.bigenc.ru/linguistics/text/2375801}
}

@article{Word2VecModelAnalysis,
author = {Jatnika, Derry and Bijaksana, Moch and Ardiyanti, Arie},
year = {2019},
month = {01},
pages = {160-167},
title = {Word2Vec Model Analysis for Semantic Similarities in English Words},
volume = {157},
journal = {Procedia Computer Science},
doi = {10.1016/j.procs.2019.08.153}
}

@inproceedings{Word2VecOriginal,
  author       = {Tom{\'{a}}s Mikolov and
                  Kai Chen and
                  Greg Corrado and
                  Jeffrey Dean},
  title        = {Efficient Estimation of Word Representations in Vector Space},
  booktitle    = {1st International Conference on Learning Representations, {ICLR} 2013,
                  Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year         = {2013},
  url          = {http://arxiv.org/abs/1301.3781},
  timestamp    = {Mon, 28 Dec 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{Kutuzov2020Thesis,
  author       = {Andrey Kutuzov},
  title        = {Distributional Word Embeddings in Modeling Diachronic Semantic Change},
  school       = {University of Oslo},
  year         = 2020,
  type         = {Doctoral Thesis},
  url          = {https://www.duo.uio.no/bitstream/handle/10852/81045/1/Kutuzov-Thesis.pdf},
  urn          = {URN:NBN:no-84130},
  language     = {English}
}

@misc{SketchEngineWSI,
  author       = {Sketch Engine},
  title        = {Word Sense Induction Guide},
  howpublished = {\url{https://www.sketchengine.eu/guide/word-sense-induction/}},
  year         = {2023},
  note         = {Retrieved May 26, 2024}
}

@inproceedings{DBSCAN,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},
location = {Portland, Oregon},
series = {KDD'96}
}

@book{ushakov1940,
  author       = {Ushakov, D.N.},
  title        = {Tolkovy slovar' russkogo yazyka. T. 4. S - Yashchurny [Explanatory Dictionary of the Russian Language. Vol. 4. S - Yashchurny]},
  editor       = {Volin, B.M. and Ushakov, D.N.},
  year         = {1940},
  address      = {Moskva},
  publisher    = {Gosudarstvennoye izd-vo inostrannykh i natsional'nykh slovarey},
}

@book{TolkovyDmitriev,
  title       = {Tolkovy slovar' russkogo yazyka: Ok. 2000 slovar. st., svyshe 12000 znacheniy [Explanatory Dictionary of the Russian Language: About 2000 Dictionary Entries, Over 12000 Meanings]},
  publisher   = {Astrel' [i dr.]},
  year        = {2003},
  editor      = {Dmitriev, D.V.},
  address     = {Moskva},
  note        = {GUP IPK Ulyan. Dom pechati},
  pages       = {989},
  size        = {15 cm},
  series      = {Slovari Akademii Rossiyskoy},
  isbn        = {5-271-06086-1},
}

@book{TolkovyKuznetsov,
  title       = {Bol'shoy tolkovy slovar' russkogo yazyka: A-Ya [Large Explanatory Dictionary of the Russian Language: A-Ya]},
  publisher   = {Norint},
  year        = {1998},
  editor      = {Kuznetsov, S.A.},
  address     = {SPb.},
  pages       = {1534},
  isbn        = {5-7711-0015-3},
  note        = {RAN. Inst. lingv. issled. Sost., gl. red. kand. filol. nauk S.A. Kuznetsov},
}

@Book{MAS1981,
  title         = {Slovar' russkogo yazyka: V 4-kh t. [Dictionary of the Russian Language: In 4 Volumes]},
  publisher     = {Russkiy yazyk},
  year          = {1981-1984},
  editor        = {A.P. Evgenyeva},
  edition       = {4-e izd., ispr. i dop [4th ed., corrected and supplemented]},
  address       = {Moskva},
  organization  = {Poligrafresursy},
  note          = {V 4-kh tomakh [In 4 volumes]},
}

@book{SemanticDefinitionsAndAnalysis,
  title       = {Slovarnye definicii i semanticheskiy analiz [Dictionary Definitions and Semantic Analysis]},
  author      = {Sternin, I.A. and Rudakova, A.V.},
  year        = {2017},
  publisher   = {Istoki},
  address     = {Voronezh},
  pages       = {34},
}

@book{TwoCenturies,
  editor        = {N.R. Dobrushina and M.A. Daniel'},
  title         = {Dva veka v dvadtsati slovakh [Two Centuries in Twenty Words]},
  edition       = {2},
  publisher     = {Izdatel'skiy dom Vysshey shkoly ekonomiki},
  year          = {2018},
  address       = {Moskva},
  isbn          = {978-5-7598-1480-1},
  url           = {https://znanium.com/catalog/product/1018929},
  urldate       = {2024-04-20},
  language      = {Russian},
  organization  = {Nats. issled. un-t «Vysshaya shkola ekonomiki»},
  type          = {Electronic Resource},
  pagetotal     = {455},
  file          = {PDF},
  accessmode    = {Subscription},
}

@book{VinogradovWordHistory,
  title        = {Istoriya slov: okolo 1500 slov i vyrazheniy i boleye 5000 slov, s nimi svyazannykh [History of Words: About 1500 Words and Phrases and Over 5000 Related Words]},
  author       = {Vinogradov, V.V. and Shvedova, N.Yu.},
  isbn         = {9785892850261},
  lccn         = {2001425036},
  url          = {https://books.google.ru/books?id=zB7YAQAACAAJ},
  year         = {1999},
  publisher    = {Institut russkogo yazyka im. V.V. Vinogradova RAN},
}

@article{Ruscorpora,
  title        = {Natsionalny korpus russkogo yazyka 2.0: novye vozmozhnosti i perspektivy razvitiya},
  author       = {Savchuk, S.O. and Arkhangelskiy, T.A. and Bonch-Osmolovskaya, A.A. and Donina, O.V. and Kuznetsova, Yu.N. and Lyashevskaya, O.N. and Orekhov, B.V. and Podryadchikova, M.V.},
  journal      = {Voprosy Yazykoznaniya},
  year         = {2024},
  volume       = {2},
  pages        = {7--34},
}
