\documentclass[LI,VKR]{HSEUniversity}
% Возможные опции: KR или VKR; LI

\title{Оценка применимости метода детектирования семантических изменений слов нейросетевой
языковой моделью на основе генерируемых определений}
\author{Татаринов Максим Дмитриевич}
\supervisor{канд. комп. н.}{А. В.~Демидовский}

%	\HasReviewertrue
\reviewer{TBD}{П.П.~Петров}

	\HasConsultanttrue
\consultant{канд. фил. н., доцент департамента фундаментальной и прикладной лингвистики}{А. Ю. ~Хоменко}

\Year{2024}
\City{Нижний Новгород}

%\Abstract{
% 	Автоматическое обнаружение семантических изменений является областью исследований,
%    вызывающей всё больший интерес из-за её важности для понимания эволюции языка и
%    предоставления данных ключевым потенциальным пользователям,
%    таким как лексикографы или историки языка.
%    Ученые пытались зафиксировать семантические сдвиги, используя несколько методов,
%    включая статические векторные представления слов, такие как Word2Vec (Фомин и др., 2019),
%    более динамичные контекстуальные представления,
%    такие как те, что создаются с помощью BERT или XML-R
%    (Родина и др., 2020; Рачинский и др., 2021).
%    Последние попытки использовали трансформеры с архитектурой кодировщика-декодировщика
%    для генерации определений в качестве представлений смысла слов (Джулианелли и др., 2023).
%    Несмотря на этот прогресс, остаётся нехватка исследований,
%    сосредоточенных на русском языке с использованием этих передовых техник.
%    Целью данного исследования является оценка применимости метода обнаружения
%    семантических изменений в русском языке с использованием определений,
%    сгенерированных языковой моделью.
%    В данном исследовании предлагается использовать последние архитектуры
%    трансформеров с кодировщиком-декодировщиком для моделирования определений для русского языка.
%    Ожидаемым результатом исследования является достижение 80\% или более корреляции с аннотациями,
%    созданными человеком, об обнаружении семантических изменений для русского языка.
%}

\addbibresource{library.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ТЕКСТ РАБОТЫ %%%%%%%%%%%%%%%
\begin{document}

% Обязательные элементы оформления: заголовочный слайд, аннотация, оглавление
\maketitle

\chapter*{Введение}

На \textbf{актуальность} настоящей работы указывают следующие факторы.
Во-первых, активное изучение темы автоматического определения семантических изменений.
В последние годы в работах использовались различные методы, включая статические эмбеддинги,
контекстуальные эмбеддинги и заканчивая генерацией определений с помощью языковых моделей
в новейших исследованиях
~~\cite{kutuzov-etal-2018-diachronic,rodina2020elmo,DefinitionGenerationMainArticle}~.
При этом, абсолютное большинство исследований, посвященных моделированию определений,
проводятся с использованием материала английского языка~\cite{DefinitionModelingReviewAndDatasetAnalysis}.
Для русского языка вопрос анализа семантических изменений на основе автоматически
сгенерированных определений недостаточно изучен.
Во-вторых, неудовлетворительное качество традиционных методов для основных потенциальных
пользователей таких технологий, таких как лексикографы, историки языка и социологов.
Например, лексикографам недостаточно данных только о факте сдвига значения, им хотелось бы
получать описания старых и новых значений слов в пригодной для чтения форме, возможно,
даже с дополнительными пояснениями.
Данная проблема может решаться моделированием определений с использованием языковых
моделей, при использовании которых исследователи смогут получить более наглядные
результаты~\cite{DefinitionGenerationMainArticle}.

\textbf{Целью} настоящей работы является оценка применимости метода детектирования семантических
изменений слов нейросетевой языковой моделью на основе генерируемых определений.

Из поставленной цели были сформулированы следующие \textbf{задачи}:
\begin{enumerate}
    \item Провести анализ существующей литературы и решений по задаче детектирования семантических
    изменений на основе генерируемых определений.
    \item Собрать датасет словарей русского языка в качестве материала для обучения модели.
%а также диахронический корпус текстов на основе НКРЯ.
    \item Обучить языковую модель на данных словарей для того, чтобы генерировать определения.
    \item Создать алгоритм автоматического определения семантических сдвигов на
основе векторного представления.
    \item Провести анализ метрик и качества обученной языковой модели и сравнить их с
    существующими решениями.
    \item Создать алгоритм визуализации результатов.
    \item Провести качественный анализ результатов работы компьютерной программы.
%    \item  Разработать прототип системы, позволяющей проводить анализ семантических изменений
%    сторонним исследователям, используя разработанный в настоящей работе алгоритм.
\end{enumerate}

\textbf{Объектом} исследования является метод детектирования семантических изменений слов.

\textbf{Предметом} исследования является применимость метода детектирования семантических изменений
слов с использованием нейросетевой языковой модели на основе генерируемых определений.

Для решения поставленных задач были использованы следующие \textbf{методы}:
\begin{enumerate}
    \item Метод анализа и синтеза для создания теоретической базы для данного исследования
на основе литературы.
    \item Компьютерный метод для написания алгоритмов программы и обучения модели.
    \item Методы обработки естественного языка для предобработки текстов.
    \item Методы глубокого обучения для алгоритма автоматического определения семантических
сдвигов на основе их векторного представления.
%    \item Метод комплексного лингвистического анализа результатов работы алгоритма.
    \item Метод лексико-семантического анализа (используется при оценке визуализаций алгоритма).
\end{enumerate}

\textbf{Новизна} настоящей работы состоит в том, что для детектирования семантических изменений
значений слов применяется на материале русского языка и с использованием SOTA-моделей.

\textbf{Практическая значимость} данной работы заключается в том, что результаты настоящей работы
можно применять для определения степени семантического сдвига лексем, с наличием визуализаций и
определений для каждого выявленного значения, что может быть использовано в лексикологии,
где необходимы актуальные данные для построения новых словарей~\cite{DefinitionGenerationMainArticle}.
Кроме того, модель, позволяющая автоматически генерировать качественные словарные определения,
может быть полезна в таких задачах обработки естественного языка, как анализ тональности,
машинный перевод и разграничение семантической неоднозначности~\cite{DefinitionModelingReviewAndDatasetAnalysis}.

В качестве \textbf{материала исследования} используется диахронический корпус НКРЯ, охватывающий
три периода (1700—1916, 1918—1991 и 1992—2016 годы) и имеющий в совокупности 250 миллионов
словоупотреблений.
Данный корпус выбран, поскольку датасет слов для валидации с изменившимся и неизменившимся значением,
использующийся для оценки алгоритма, основан на данном корпусе~\cite{rushifteval}.
Корпус был получен по запросу к авторам НКРЯ.

\textbf{Апробация работы.}
Основные положения настоящей работы были представлены на конференции
(VIII Всероссийская научная студенческая конференция НИУ ВШЭ – Нижний Новгород
«Цифровые технологии в современной молодежной науке», 17 апреля 2024 г.,
тема доклада: «Оценка применимости метода детектирования семантических изменений слов нейросетевой
языковой моделью на основе генерируемых определений»).

\chapter{Теоретические аспекты автоматического выявления семантических изменений}

\section{Понятия и классификации}

В рамках изучения исторических изменений в лексике языка или языков, лингвисты оперируют
такими понятиями, как лексические изменения, семантические изменения,
грамматикализация и лексическая замена~\cite{TahmasebiComputationalApproachesToSemanticChange}.

Лексические изменения в широком смысле охватывают все виды диахронических преобразований
в словарном составе языка, в то время как в более узком значении термин относится к
появлению новых форм в языке, таких как заимствованные слова и неологизмы,
а также к устаревшим словам.

Семантические изменения или семантический сдвиг являются особым случаем лексических изменений,
когда существующая форма (лексема) приобретает или теряет конкретное значение,
что приводит к увеличению или уменьшению полисемии.

Примером таких изменений может служить эволюция английских слов, когда ранее специализированное
слово для обозначения определенного вида собаки стало общим термином (dog),
в то время как более раннее общее слово для «собаки» — современный аналог которого
hound — сейчас используется для обозначения специального вида собак.

Грамматикализация описывает особый вид семантических изменений, когда слова с
полным значением превращаются в служебные слова и, в конечном итоге,
в связанные грамматические морфемы.
Примером может служить развитие глагольного аффикса \textit{-ся} из
безударного возвратного местоимения формы винительного падежа.

В рамках настоящей работы мы будем заниматься исследованием семантических изменений
лексического значения слов, не касаясь иных схожих явлений.

Чтобы подробно разобрать изменения семантики слова,
полезно обратиться к типологии,
разработанной американским лингвистом Леонардом Блумфилдом~~\cite{BloomfieldLanguage,
    Harris2014SemanticShifts}.
Он выделил следующие типы:

\begin{enumerate}
    \item Сужение значения (narrowing), при котором слово начинает употребляться
в более узких сферах общения за счет конкретизации своего значения.
Например, Old English mete, означавшее «еда», сузилось до современного английского meat,
означающего «мясо».
    \item Расширение значения (widening), при котором значение слова становится более общим,
способным применяться во многих сферах общения.
Например, слово dog в Middle English означало «собаку определенной породы»,
но теперь описывает «любую собаку».
    \item Гипербола (hyperbole), при которой значение слова нарочно преувеличивается.
Например, слово из допрефранцузского *ex-tonare, означавшее «ударить громом»,
преобразовалось в французское étonner, означающее «удивлять».
    \item Литота (litotes), когда значение слова нарочно подвергается преуменьшению.
Например, Old English cwellan, которое означало «убить», произошло от
предшествующего *['kwalljan], означавшего «пытать».
    \item Деградация значения (degeneration), когда значение слова приобретает
более негативное значение в течение времени.
Например, Old English cnafa, первоначально обозначало «мальчик, слуга», и превратилось в knave
(лжец).
    \item Возвышение значения (elevation), при котором значение слова приобретает
более позитивное значение в течение времени.
Например, knight произошло от Old English cniht, которое означало «мальчик, слуга».
    \item Метафора (metaphor), заключающаяся в изменении значения на основе скрытого сравнения.
Например, прагерманское *['bitraz], означавшее «колючий», превратилось в bitter,
означающее «горький».
    \item Перенос значения, или метонимия (metonymy), на основе смежности
или близости ассоциативных связей.
Например, Old French joue, означавшее «щека», стало означать «челюсть».
    \item Синекдоха (synecdoche), когда значение слова представляет отношение
части к целому или наоборот.
Например, прагерманское *['tu:naz] означало «забор» и превратилось в английском в «небольшой город».
\end{enumerate}

\section{Обзор существующих методов}

\subsection{Предыстория}

Традиционно для изучения изменений семантики слов использовались ручные методы детального
анализа текстов.
Из существующих исследований истории значений слов в русском языке можно привести
исследование 1500 слов и 5000 связанных с ними выражений В.В. Виноградова~\cite{VinogradovWordHistory}, а также
книгу «Два века в двадцати словах», в деталях описывающую историю набора двадцати слов.~\cite{TwoCenturies}
Хотя ручные методы продолжают применяться в лингвистике, в последнее время появилось
множество полуавтоматических и автоматических методов,
способных расширить и углубить такие исследования, а также упростить их проведение.
Одним из факторов, позволивший предлагать такие решения стала цифровизация документов
в различных областях и появление диахронических корпусов.~\cite{TahmasebiComputationalApproachesToSemanticChange}.

\subsection{Cтатические эмбеддинги}

До 2020 года в работах чаще используют статические эмбеддинги.

Для русского языка в качестве примера можно привести проект Shiftry~\cite{shiftry},
в котором для анализа семантических сдвигов использовались модели Word2Vec,
выровненные методом Прокруста.
Эти модели были обучены на обширном корпусе русскоязычных новостных текстов,
охватывающем период с 2010 по 2020 годы, и позволили отследить диахронические изменения
в употреблении слов.
Для подсчета степени семантического сдвига использовалось косинусное расстояние между
векторами из различных временных срезов.

Статические эмбеддинги оставались наиболее актуальными вплоть до 2020,
где показывали лучшие результаты в SemEval-2020 Task 1~\cite{semeval2020task}.
Они эффективно моделируют значение слов в зависимости от обучающего корпуса
без опоры на объемные предобученные модели,
превосходя по этому качеству модели, основанные на совстречаемости слов.
Среди недостатков можно отметить необходимость большого объема слов в корпусах
для стабильности эмбеддингов;
необходимость выравнивания моделей, обученных на отдельных наборах данных,
соответствующим временным срезам, что может вносить шум;
кроме того, они моделируют среднее значение слова на основе его употребления в корпусе,
не позволяя различать разные значения слова.

\subsection{Контекстуализированные эмбеддинги}

Статические модели вложений слов присваивают каждому слову (лемме) один и тот же вектор
независимо от контекста, в то время как современные достижения в области обработки
естественного языка позволили разработать модели,
обеспечивающие получение контекстуализированных представлений высокого качества.
Данные модели отличаются тем, что на этапе вывода токенам присваиваются различные вложения
в зависимости от их контекста, что позволяет проводить более глубокий диахронический
анализ языковых изменений с использованием контекстуализированных векторных представлений слов.

Применение контекстуализированных векторных представлений задало новый стандарт для
высококачественных, чувствительных к контексту представлений в обработке естественного языка.
В статье, где исследователи использовали предварительно обученные модели BERT и ELMo,
настроенные на полном корпусе Русского национального корпуса, было обнаружено,
что эти модели показывают значительную корреляцию с человеческими оценками
при определении диахронического семантического изменения слов в русском языке~\cite{rodina2020elmo}.
Использовались алгоритмы, такие как косинусное сходство по прототипам слов и методы кластеризации,
для выявления семантических сдвигов.

Одни из последних работ по теме автоматического выявления семантических сдвигов для русского языка
были написаны в рамках соревнования RuShiftEval, прошедшего в 2021 году~\cite{rushifteval}.
В ходе него участники должны были рассмотреть три исторических периода русского языка и общества:
предсоветский (1700-1916), советский (1918-1990) и постсоветский (1992-2016).
Исследование базировалось на наборе данных RuShiftEval, который состоит из
111 русских существительных (99 в тестовом наборе и 12 в наборе для разработки),
вручную аннотированных по степени изменения их значения в трех парах временных периодов.

Аннотаторам предлагалось оценить семантическую связь значений целевого слова в двух предложениях
из разных временных периодов.
Оценки (от 1 до 4) отражали степень семантического родства между значениями слова, где
1 обозначало отсутствие связи между значениями, а 4 – их совпадение.
Затем индивидуальные оценки усреднялись, формируя общую меру семантической родственности между
употреблениями слова в разные временные периоды.
Такая задача как правило называется Word-in-Context или WiC.

Для каждого из 99 целевых русских слов участники должны
были представить три значения, соответствующих семантическому изменению в упомянутых парах
временных периодов.
Эти значения использовались для построения трех ранжирований:
RuSemShift1, RuSemShift2 и RuSemShift3.
В качестве метрики оценки использовалась ранговая корреляция Спирмена между ранжированием слов,
сгенерированным системой, и золотым ранжированием, полученным в ручной аннотации.

Победители вышеупомянутого соревнования (команда GlossReader) указывают,
что проблемой в существующих решениях являлось то,
что эмбеддинги несут в основном информацию о форме слова, а не значении~\cite{GlossReader}.
Чтобы решить это, они дообучали модель XLM-R на задаче генерации эмбеддингов, максимально близким
к таким, какие получены на соответстующим использованиям слов словарным определениям
~\cite{XLM-R}.

При дообучении их система включает в себя два отдельных энкодера на основе XLM-R:
Энкодер контектов для кодирования предложения с целевым словом и
энкодер глоссов для кодирования определения слова.
Система оценивает возможные значения смысла слова путём сравнения векторных представлений слова
и его определений.
При этом для обучения использовались данные только по английскому языку,
но модель также показала хорошие результаты для русского языка.

Далее, исследователи получали эмбеддинги контекстов слов с помощью
дообученного энкодера контекстов, высчитывали расстояние с помощью различных метрик расстояния,
самым эффективным из которых были евклидово расстояние с нормализацией, после чего
логистическая регрессия приводила значения к формату в датасете.

Авторы статьи предоставляют доступ к части исходного кода их исследования~\cite{GlossReaderGitHub}.

Так, были опубликованы следующие компоненты:
\begin{enumerate}
    \item Код, предназначенный для генерации прогнозов на основе заранее вычисленных эмбеддингов,
полученных с использованием модели.
    \item Код для оценки результатов.
\end{enumerate}

В то же время, авторы исследования не представили в открытый доступ следующие части:
\begin{enumerate}
    \item Код для предварительного обучения модели.
    \item Код, позволяющий осуществлять инференцию для получения контекстуализированных
    эмбеддингов, сформированных на основе предварительно обученной модели.
\end{enumerate}

В соответствии с инструкциями, данными авторами, мы запустили доступный код,
в следствие чего были полуены высокие результаты, совпадающие с тем, что сообщают
авторы в своей работе:

\begin{table}[H]
\centering
\caption{Коэффициэнты корреляции}
\begin{tabular}{lc}
\hline
Пары периодов                  & Коэффициент корреляции \\
\hline
Среднее            & 0.8021                  \\
pre-Soviet:Soviet           & 0.7808                  \\
Soviet:post-Soviet          & 0.8032                  \\
pre-Soviet:post-Soviet      & 0.8223                  \\
\hline
\end{tabular}
\end{table}

Среди недостатков работы можно отметить неспособность модели корректно выявлять
значения тех слов, которые отличаются от ближайших аналогов в английском, например,
«пионер», связанный с коммунистической идеологией и не соответствующий в полной мере
слову «scout».

Кроме того, команда DeepMistake представила решение, занявшее в соревнование второе место
~\cite{DeepMistake}.
Однако, они смогли доработать его и повысить результаты до первого уже после окончания
соревнования.

Исследователи обучали модель XLM-R на обширном многоязычном датасете Word-in-Context,
а затем дооубчали ее на наборе данных RuSemShift для настоящей задачи,
приводит к наилучшим результатам.
В отношении архитектуры авторы утверждают, что применение линейного слоя на верхнем уровне,
основанного на объединении L1-метрики и скалярного произведения между контекстуализированными
эмбеддингами XLM-R, показывает лучшую производительность по сравнению с
более традиционными подходами, такими как конкатенация эмбеддингов и
использование нелинейных классификаторов.

Исследователи выложили исходный код полностью и предлагают возможность воспроизвести их
результат~\cite{DeepMistakeGitHub}.
Значени метрик, сообщенные исследователями, воспроизводятся.

\begin{table}[H]
\centering
\caption{Коэффициэнты корреляции с использованием IsoReg}
\begin{tabular}{lc}
\hline
Пары периодов                  & Коэффициент корреляции \\
\hline
Среднее            & 0.8494                  \\
pre-Soviet:Soviet           & 0.8563                  \\
Soviet:post-Soviet          & 0.841                  \\
pre-Soviet:post-Soviet      & 0.8511                  \\
\hline
\end{tabular}
\end{table}

Среди недостатков статьи можно выделить то, что авторы не предоставляют возможность визуализации
или интерпретации результатов, кроме непосредственно получившегося значения метрики.

Тем не менее, применимость таких методов была подвергнута сомнению в работе
Giulianelli et al.~\cite{DefinitionGenerationMainArticle},
где ставится под сомнение широкая практичность ранее упомянутых подходов.
Они утверждают, что такие методы практически неинтерпретируемы,
поскольку они не дают описаний значений слов,
а лишь бинарные результаты наличия или отсутствия семантического изменения.
Исследование, которое в наибольшей степени занимается этой проблемой, - это GlossReader,
где исследователи предлагают способ визуализации и интерпретации результатов.
Однако у этого метода есть свои недостатки, обсуждаемые выше.
Учитывая эти факты, новый подход, включающий моделирование определений,
вызывает интерес для задачи обнаружения семантических изменений.

Начало интереса к моделированию определений как теме исследования
в области обработки естественного языка можно отнести
к работе Noraset et al.~\cite{noraset2016definition}.
Они были среди первых, кто исследовал потенциал использования векторных представлений слов
для автоматической генерации определений.
Изначально была поставлена упрощенная задача с моносемантическими словами,
которые, как правило, имеют одно значение и, следовательно, одно определение.
Однако оставалась нерешенной проблема многозначных слов.
Gadetsky et al. выделили важное условие для моделирования определений:
необходимость контекста для точного захвата нюансов языка~\cite{gadetsky-etal-2018-conditional}.
В своем исследовании они включили примеры предложений для предоставления контекста модели,
что оказалось решающим шагом в возможности модели справляться с полисемией и улучшении
ее производительности.

Несмотря на достижения, сделанные вышеупомянутыми исследователями,
область моделирования определений все еще сталкивалась с значительными проблемами.
Huang et al. выявили наличие таких проблем,
как проблема слов вне словаря, когда модели сталкиваются с трудностями в работе со словами,
не встречавшимися во время обучения, а также проблемы избыточной и недостаточной
специфичности в определениях~\cite{huang-etal-2021-definition}.
Исследователи сообщают: «Избыточно специфичные определения представляют узкие значения слов,
в то время как недостаточно специфичные определения представляют общие и
нечувствительные к контексту значения.»
Huang et al. решили эти проблемы,
используя предварительно обученную модель энкодера-декодера,
а именно Text-to-Text Transfer Transformer (T5),
и ввели механизм ранжирования, предназначенный для тонкой настройки специфичности
генерируемых определений.
Метод был протестирован на стандартных наборах данных для оценки и показал значительное
улучшение по сравнению с предыдущими методами.

Самой актуальной работой по теме использования сгенерированных большими языковыми моделями
определений для автоматического выявления семантических изменений является статья
Giulianelli et al.~\cite{DefinitionGenerationMainArticle}.

Авторы определяют задачу генерации определений следующим образом: для заданного слова w и примера
использования s (предложения, содержащего w) необходимо сгенерировать определение d на
естественном языке, которое будет грамматически корректным и точно передавать значение слова
w в контексте его использования.
Для генерации определений они используют модель Flan-T5, версию трансформера T5,
дополнительно обученную на 1,8 тысячах задач по обработке естественного языка.
Ниже вы можете видеть пример работы модели.

\begin{table}[H]
\centering
\caption{Пример определения, сгенерированного моделью Flan-T5 XL}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Пример использования} & ‘Примерно половина солдат в наших стрелковых взводах были призывниками, которых мы обучали около шести недель.’ \\
\hline
\textbf{Целевое слово} & призывник \\
\hline
\textbf{Сгенерированное определение} & ‘ЧЕЛОВЕК, КОТОРЫЙ ПОДЛЕЖИТ ПРИЗЫВУ В ВООРУЖЕННЫЕ СИЛЫ’ \\
\hline
\end{tabular}
\end{table}

Первым шагом исследователи выбирают, используя метрики BLEU, NIST, BERTScore, наиболее
подходящий под задачу промпт из нескольких вариантов, например
\("\)what is the definition of <trg>?\("\) или \("\)define the word <trg>\("\).

Для дообучения модели авторы используют три датасета, каждый из которых содержит определения
слов, сопровождаемые примерами употребления: WordNet, данные Оксфордского словаря и CoDWoE,
основанный на определениях и примерах, извлеченных из Викисловаря.

Для оценки качества модели исследователи используют метрики SacreBLEU, ROUGE-L и BERT-F1.

Для демонстрации работы со сгенерированными определениями авторы
работы используют датасет, в котором слова представлены в графах диахронного использования
слов (Diachronic Word Usage Graphs, DWUG), взвешенных, ненаправленных графах,
узлами которых служат примеры использования слов, а веса рёбер отражают семантическую
близость пар употреблений.
DWUG созданы на основе многоэтапного процесса человеческой аннотации, в ходе которого аннотаторы
оценивали семантическую связность пар употреблений слов по 4-балльной шкале.

Прежде всего, авторы исследования проводят анализ корреляции между близостью пар слов в DWUG
и контекстуальными эмбеддингами токенов, эмбеддингами предложений примеров использования, а также
сгенерированными определениями.
Результаты показали, что сгенерированные определения обладают более высокой степенью
корреляции с данными из DWUG, чем традиционно полученные эмбеддинги.

Далее исследователи анализируют пространство эмбеддингов определений слов,
чтобы выяснить, как они могут помочь в различении разных значений слов.
Они обнаружили, что эмбеддинги определений образуют более плотные и четко определенные
кластеры по сравнению с эмбеддингами токенов и примеров предложений, что делает их
подходящими для представления значений слов.

Далее авторы присваивали кластерам, полученным на основе данных из DWUG,
соответствующие им определения.
Для обобщения определений в одном кластере авторы использовали самое прототипическое из них.
Они представляли все определения с помощью их эмбеддингов предложений и выбирали в качестве
прототипичного определение, эмбеддинг которого наиболее похож на среднее значение всех
эмбеддингов в кластере.

Авторы приходят к выводу, что сгенерированные определения слов могут играть роль
семантического представления слов, аналогичному традиционным эмбеддингам.
Они находят большие языковые модели достаточно развитыми для генерации определений
простым промптом.
При этом полученные таким образом определения превосходят по качеству
традиционные эмбеддинги и являются более наглядными.

\chapter{Имплементация автоматического выявления семантических изменений}

\section{Обучение языковой модели на данных тезауруса}

В качестве модели была выбрана FRED-T5-1.7B, являющаяся одной из новейших языковых моделей,
выпущенных SberDevices и обученных с нуля на материале русского языка~\cite{FRED-T5}.
Для выбора модели мы использовали бенчмарк для оценки продвинутого понимания русского языка
\("\)RussianSuperGLUE\("\)~\cite{RussianSuperGLUE}.В бенчмарке присутствуют шесть групп задач, охватывая
общую диагностику языковых моделей и различные лингвистические задачи: понимание здравого смысла, логическое
следование в естественном языке, рассуждения, машинное чтение и знания о мире.
FRED-T5-1.7B занимает самое высокое место в лидерборде данного бенчмарка, со значением 0.762,
уступая лишь результатам выполнения данных заданий людьми со значением 0.811,
что свидетельствует о ее способности к выдающемуся языковому пониманию и анализу.
Таким образом, FRED-T5-1.7B представляется наиболее подходящей языковой моделью
для задачи генерации определений.

Одной из ключевых особенностей модели FRED-T5-1.7B является наличие денойзеров.
Денойзеры — это специальные механизмы, задача которых состоит в очистке текста от шума,
то есть в восстановлении удаляемых или искажаемых частей текста.
В модели используется семь различных денойзеров, каждый из которых выполняет
уникальную функцию в процессе обучения.
Основные задачи денойзеров включают в себя восстановление
удаленных участков текста, а также продолжение текстовых последовательностей.

В настоящей работе при работе с моделью используется денойзер,
помеченный спецтокеном \("\)<LM>\("\),
который задействован в задаче продолжения последовательности текста.

Действия, описанные далее, подкрепляются кодом, выложенным в открытый доступ, на сайте GitHub
и могут быть воспроизведены.~\cite{WorkGitHub}

В качестве материала, используемого для обучения модели, выступил датасет словарей,
который включал в себя материал из русской версию Викисловаря~\cite{Wiktionary}, а также из МАС
«Малого академического словаря»~\cite{MAS1981}.
Материал Викисловаря получен с помощью написанного скрипта на языке Python, позволяющего
извлечь данные из выгрузки Викисловаря в формат JSONL.
Материал Малого академического словаря был получен с помощью скраппера, написанного на языке Python.
В загруженном наборе данных в каждом вхождении присутствовали
идентификатор статьи, лексема, про которую написана данная статья, а также определения
с примерами использования.

\begin{longtable}{|m{3cm}|m{10cm}|}
\caption{Информация о лексеме из Викисловаря} \\
\hline
\textbf{Лексема} & \textbf{Определения и примеры использования} \\
\hline
\endfirsthead
\multicolumn{2}{c}%
{\tablename\ \thetable\ -- \textit{Продолжение с предыдущей страницы}} \\
\hline
\textbf{Лексема} & \textbf{Определения и примеры использования} \\
\hline
\endhead
\hline \multicolumn{2}{r}{\textit{Продолжение на следующей странице}} \\
\endfoot
\hline
\endlastfoot
прозябнуть & \textbf{сильно озябнуть, промёрзнуть:} Я и без того прозяб,
инстинкт тянет меня согреться, а какой-то нелепый долг повелевает лезть в холодную воду.
Усталость возьмет свое, тогда можно жестоко прозябнуть и опасно заболеть.
\newline \textbf{прорасти:} Сперва надо его в землю посадить, потом ожидать,
покуда в нем произойдет процесс разложения, потом оно даст росток, который прозябнет,
в трубку пойдет, восколосится и т. д. \\
\end{longtable}

Полученный материал был очищен от вхождений, не имеющих при себе примеров использования,
информативных определений, например, «Состояние по знач. глаг. линять»,
или не содержащих определений вовсе, а также имеющие такие определения,
которые представляют грамматическую информацию о слове вместо лексического значения,
например, «наречие к причастию приглашающий».
На выходе было получено 270 тысяч 555 вхождений.

Примеры и слова были отформатированы под формат запроса модели.
В начале после слова «Контекст» шел пример использования слова, после чего шла фраза
«Определение слова», в которую включалось само слово.
Таким образом, на вход модель
принимает лексему и контекст, в которой она употреблялась, а на выход ожидается сгенерированное
определение.

\begin{table}[h]
\centering
\caption{Пример отформатированного запроса модели}
\begin{tabular}{|m{2.5cm}|m{9.5cm}|}
\hline
\textbf{Поле}       & \textbf{Значение}                                                                                          \\
\hline
input\_text  & <LM>Контекст: \("\)Усталость возьмет свое, тогда можно жестоко прозябнуть и опасно заболеть.\("\) Определение слова \("\)прозябнуть\("\): \\
\hline
target\_text & Сильно озябнуть, промёрзнуть.                                                                              \\
\hline
\end{tabular}
\end{table}

FRED-T5-1.7B была дообучена на полученном из Викисловаря материале в течение трёх эпох
с постоянным шагом обучения 0.001, размером батча 16 и оптимизатором Adafactor
на одной видеокарте RTX 3090.
Для ускорения обучения и экономии видеопамяти использовалась технология LoRa со
следующими параметрами: r – 32, alpha – 64, dropout – 0.1, что позволило
уменьшить количество обучаемых параметров до 14155776 (0.8\% от общего числа параметров),
что поволило сэкономить используемую видеопамять и ускорить обучение.
Более подробный обзор гиперпараметров модели, а также хода ее обучения доступен в приложении А.

Для оценки качества обучения модели используются метрики BLEU и ROUGE-L,
которые оценивают формальную схожесть текста: BLEU оценивает точность совпадений n-грамм
в сгенерированном тексте по сравнению с эталонным текстом~\cite{BLUE}, а ROUGE-L измеряет схожесть между
сгенерированным текстом и эталонным текстом на основе наибольшей общей последовательности слов~\cite{ROUGE}.
Также использовалась метрика BERT-F1, учитывающая семантику сравниваемых текстов благодаря
использованию контекстуальных эмбеддингов при подсчете значения метрики~\cite{BERTScore}.
Использование нескольких метрик позволяет получить более полную картину качества модели,
поскольку каждая из них оценивает разные аспекты сгенерированного текста.
Как традиционные BLEU и ROUGE-L, так и более современный BERT-F1 активно используются в
задачах обработки естественного языка, в том числе в задачах генерации текста.
В данной работе использовались версии этих инструментов, взятые из библиотеки evaluate~\cite{Evaluate}.
Так, в обзорной статье по моделированию определений утверждается, что на момент выпуска статьи BLUE
использовался в 9 научных публикациях, ROUGE-L и BERTScore – в 3~\cite{DefinitionModelingReviewAndDatasetAnalysis}.
Кроме того, данные метрики используются и в более новых работах.
Так, в настоящей статье результаты данных метрик будут сравниваться с таковыми из статьи
Giulianelli M. et al., где сообщаются результаты трёх вышеперечисленных метрик при обучении модели
Т5 для задаче генерации определений на английском языке~\cite{DefinitionGenerationMainArticle}.

\begin{table}[H]
\centering
\caption{Результаты дообучения FRED-T5-1.7B на датасете Викисловаря}
\begin{tabular}{lc}
\hline
Метрика                  & Значение \\
\hline
BLEU            & 8.3162                  \\
ROUGE-L           & 37.45                  \\
BERT-F1          & 78.06                  \\
\hline
\end{tabular}
\end{table}

\section{Тестирование модели на материале соревнования Rushifteval}

С помощью модели были получены определения для тестовой части датасета соревнования Rushifteval.

Для векторизации сгенерированных определений использовалась paraphrase-multilingual-mpnet-base-v2,
векторы были нормализированы, после чего расстояние между
векторным представлением определений считалось с помощью косинусного расстояния.
Результат приводился в формат значений датасета с помощью линейной регрессии,
тренированной на датасете Rusemeval.

\begin{table}[H]
\centering
\caption{Коэффициэнты корреляции с использованием IsoReg}
\begin{tabular}{lc}
\hline
Пары периодов                  & Коэффициент корреляции \\
\hline
Среднее            & 0.72                  \\
pre-Soviet:Soviet           & 0.707                  \\
Soviet:post-Soviet          & 0.731                  \\
pre-Soviet:post-Soviet      & 0.723                  \\
\hline
\end{tabular}
\end{table}

Рассмотрим данные результаты в сравнении с аналогами из соревнования Rushifteval.

\begin{table}[H]
\centering
\caption{Результаты алгоритма в сравнении с результатами команд Rushifteval.}
\begin{tabular}{|m{2.5cm}|m{3cm}|m{3cm}|m{3cm}|m{2.0cm}|}%m{1.5cm}|}
\hline
\textbf{Команда} & \textbf{досоветский:
советский} & \textbf{советский:
постсоветский} & \textbf{досоветский:
постсоветский} & \textbf{Среднее} \\% & \textbf{Тип} \\
\hline
GlossReader & 0.781 & 0.803 & 0.822 & 0.802 \\% & token \\
\hline
DeepMistake & 0.798 & 0.773 & 0.803 & 0.791 \\% & token \\
\hline
vanyatko & 0.678 & 0.746 & 0.737 & 0.720 \\% & token \\
\hline
\textbf{FRED-T5-FN} & \textbf{0.707} & \textbf{0.731} & \textbf{0.723} & \textbf{0.72} \\% & \textbf{token} \\
\hline
aryzhova & 0.469 & 0.450 & 0.453 & 0.457 \\% & token \\
\hline
Discovery & 0.455 & 0.410 & 0.494 & 0.453 \\% & token \\
\hline
UWB & 0.362 & 0.354 & 0.533 & 0.417 \\% & type \\
\hline
dschlechtweg & 0.419 & 0.373 & 0.383 & 0.392 \\% & type \\
\hline
jenskaiser & 0.430 & 0.310 & 0.406 & 0.382 \\% & type \\
\hline
SBX-HY & 0.388 & 0.281 & 0.439 & 0.369 \\% & type \\
\hline
Baseline & 0.314 & 0.302 & 0.381 & 0.332 \\% & type \\
\hline
svart & 0.163 & 0.223 & 0.401 & 0.262 \\% & type \\
\hline
BykovDmitrii & 0.274 & 0.202 & 0.307 & 0.261 \\% & token \\
\hline
fdzr & 0.217 & 0.251 & 0.065 & 0.178 & \\% type \\
\hline
\end{tabular}
\end{table}

Как видно из таблицы, настоящее решение лучше по качеству
большинства аналогов из соревнования Rushifteval,
хоть и уступает некоторым, использующим модели XLM-R.
Два решения с самым высоким качеством были описаны ранее в Главе 1.

\section{Визуализация результатов работы модели}

Для создания визуализаций семантических изменений слов используются используются библиотеки
Matplotlib и Scikit-learn.
Полученные с помощью модели определения векторизуются по аналогии с выше описанными главами.
Так как для слов, имеющих одинаковое значение,
модель склонна генерировать семантичсеки близкие, однако не идентичные дословно определения,
для группировки таких схожих определений применяется алгоритм кластеризации DBSCAN из
библиотеки Scikit-learn на основе векторных представлений.
Алгоритм кластеризации может настраиваться вручную через два ключевых параметра:
\("\)eps» и «min\_samples\("\).
Параметр «eps» определяет максимальное расстояние между двумя точками,
чтобы они считались находящимися в одном соседстве.
\("\)Min\_samples» определяет минимальное количество точек,
которые должны образовывать плотно связанную группу, чтобы она образовывала кластер.
Затруднительно сказать заранее, какие параметры кластеризации подойдут для визуализации
каждого конкретного слова.
Представляется хорошим вариантом сначала выбирать небольшие значения и после повышать их,
пока значения, сформулированные похожим, но разным образом, не объединяться в единые кластеры.
После этого, для каждого полученного кластера выбирается прототипическое определение,
векторное представление которого наиболее близко к центру кластера.
Данное определение выбирается для описания данного значения (кластера).
Затем библиотека Matplotlib применяется для создания столбиковых диаграмм,
отражающих частоту употреблений различных значений слова во времени,
и для обеспечения наглядности с помощью цветовой градации и легенд,
содержащих прототипические определения каждого из значений.

Результатом анализа является график по типу иллюстрации, где представлена столбчатая диаграмма,
показывающая процентное соотношение значений исследуемого слова за разные периоды времени.
Каждая категория обозначена на диаграмме своим цветом и соответствующим временным интервалом:
светло-синий цвет для 1700-1916, средне-синий для 1916-1990 и темно-синий для 1992-2017.
Под диаграммой находится расшифровка значений, а также использованные параметры визуализации.

\begin{FIGURE}[H]{Изменение значений слова "Свалка" \label{fig:example-figure-2}}
	\includegraphics[width=0.75\textwidth]{img/Words_Meanings/Figure_Свалка (Больше значений)}
\end{FIGURE}

%Как видно в примере на иллюстрации 2.1, визуализация слов из набора данных Rusemshift
%позволяет наглядно выявлять семантические различия в словах в разных эпохи.
%
%\begin{FIGURE}[H]{Изменение значений слова "Спутник" \label{fig:example-figure-2}}
%	\includegraphics[width=1.0\textwidth]{img/Figure_Sputnik_2}
%\end{FIGURE}

\chapter{Анализ результатов работы модели}

Для дальнейшего анализа результатов алгоритма использовались 20 слов с изменившемся
значением из книги «Два века в двадцати словах»~\cite{TwoCenturies}.
Использования данных слов брались из диахронического корпуса НКРЯ.

Из каждого периода (досоветсткий, советский и постсоветсткий) бралась выборка из 300 вхождений,
где для каждого использования слова генерировалось определение, а после строился график по аналогии с
описанием визуализации выше.

Далее для каждого слова описана семантика слов на основе словарей в соответствии
с рекомендациями издания И.А. Стернина~\cite{SemanticDefinitionsAndAnalysis}.
В качестве материала будут взяты «Большой толковый словарь» (далее \textit{БТС})~\cite{TolkovyKuznetsov},
«Толковый словарь русского языка Дмитриева» (далее \textit{ТСРЯ})~\cite{TolkovyDmitriev} и
книга «Два века в двадцати словах».
После чего будет проведено сравнение выявленных при семантическом описании лексемы
значений и тех, что выявлены алгоритмом.

Кроме того, произведено сравнение статистической информации по использованию слов
в разные периоды для значений, соотносимых со значениями из книги
«Два века в двадцати словах».

Следует учитывать то, что в книге исследуются периоды длиной меньше, чем в настоящей работе.
Например, вместо досоветского выделяют 1800-1849, 1850-1874, 1875-1899, а также 1900-1924,
в связи с чем не представляется возможным выявить изменения между короткими периодами из книги.

Рассмотрим 20 слов внимательнее.

\section*{Свалка}

\subsection*{Семантическое описание лексемы \textit{Свалка} по словарям}

В результате анализа семем лексемы \textit{свалка} в толковых словарях были
выделены семь групп значений, которые можно условно сформулировать
следующим образом:

\begin{enumerate}
    \item Место для сбора мусора, нечистот.
(\textit{«Место, куда свозят, выбрасывают мусор, нечистоты, негодные вещи.»} в БТС,
\textit{«Место, куда вывозят, выбрасывают мусор, нечистоты, негодные вещи.»} в ТСРЯ,
\textit{«Место для сбора мусора, нечистот»} в «Два века в двадцати словах»)
    \item Процесс сваливания.
(\textit{«к Свалить»} в БТС,
\textit{«Процесс сваливания»} в «Два века в двадцати словах»)
    \item Всеобщая драка.
(\textit{«Свалкой называют всеобщую драку, в которой участвует много людей»} в ТСРЯ,
\textit{«Драка»} в «Два века в двадцати словах»)
    \item Скопление людей, толпа.
(\textit{«Скопление людей, толпа.»} в «Два века в двадцати словах» и в БТС)
    \item Груда, куча, нагромождение чего-либо.
(\textit{«Беспорядочно накиданная груда, куча чего-л.»} и
\textit{«Если кто-либо превращает квартиру в свалку, то это означает, что там в беспорядке нагромождаются предметы, мебель и пр.»} в БТС,
\textit{«Свалкой называют беспорядочно накиданную груду каких-либо предметов.»} в ТСРЯ,
\textit{«Груда»} в «Два века в двадцати словах»)
    \item Битва.
(\textit{«Битва»} в «Два века в двадцати словах»)
\end{enumerate}

\subsection*{Результат алгоритма}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Свалка (Больше значений)}
	\caption{Изменение значений слова \textit{Свалка} (Параметры: eps=0.9, min\_samples=50)}
	\label{fig:Свалка}
\end{figure}

Значения для визуализации слова \textit{Свалка}:

\begin{enumerate}
    \item Место, куда свозят, сваливают мусор, отходы.
    \item Столкновение, драка.
    \item Беспорядочное скопление кого-либо, чего-либо.
    \item Место, где свалены, нагромождены какие-л. предметы.
    \item Беспорядочное, беспорядочное скопление людей.
\end{enumerate}

\subsection*{Анализ результатов}

Первые четыре определения корректно сформулированы.
Пятое имеет повторение слова «беспорядочное» – ошибку в генерации определения.
Далее мы будем считать это определение как «Беспорядочное скопление людей.».

Три определения имеют явные аналоги в составленном ранее описании значений слова.
’Место, куда свозят, сваливают мусор, отходы.’ имеет с
’Место, куда свозят, выбрасывают мусор, нечистоты, негодные вещи.’
общие смысловые элементы:
«место», «своз/сваливание», «отходы/нечистоты».

’Столкновение, драка.’ соответствует определению
’Всеобщая драка, потасовка.’,
хоть в словаре оно и более узкое из-за наличия помимо смыслового элемента
«драка» семы «всеобщности».

’Беспорядочное скопление людей.’ имеет аналог
’Скопление людей, толпа.’.
Здесь общие семы «скопление», «люди», хоть и имеется дифференциальная сема «беспорядочности»,
делающая определение алгоритма более узким.

’Место, где свалены, нагромождены какие-л. предметы.’ близко к
’Беспорядочно накиданная груда, куча чего-л.’
Общими семами являются
«нагромождение», «предметы», однако не акцентируется
«беспорядочность» явно, вместо этого эта сема присутствует в значениях слова
«нагромождены» («Нагромоздить –
построить в чрезмерно большом количестве, очень тесно или в беспорядке.»).

Также алгоритм предлагает более общее значение
’Беспорядочное скопление кого-либо, чего-либо.’,
которое может включать как живые, так и неживые объекты, объединяя
значения ’Скопление людей, толпа.’ и ’Груда, куча, нагромождение чего-либо.’

Алгоритм не предлагает значение ’Процесс сваливания’.
В этом случае акцентируется действие «свалить», указывающее на процесс перемещения
предметов/материала с целью создания свалки.
Однако, это значение не вынесено отдельно ни в одном периоде в «Двух веках в двадцати словах»
и является редким, что могло быть причиной отсутствия в визуализации.

Кроме того, в визуализации отсутствует значение ’Битва’,
которое указывается как преобладающее для периода до 1850 года.
В предсказаниях модели присутвуют примеры с этим значением,
однако их количество в исследуемом материале незначительно,
поэтому оно не вошло в визуализацию.
Например, для \textit{«Но в свалке, как обыкновенно действует кавалерия, сабля или палаш лучше»}
было сгенерировано определение
’Бой, в котором участвуют несколько противников’.

%Турки стали отступать после сильной свалки, но, получив подкрепление и пользуясь своим превосходством (пушки и конница), перешли было опять в наступление.
%Беспорядочное столкновение, бой.

Перейдем к частотности значений.

В книге «Два века в двадцати значениях» как появившееся в 1900-ых годах указано
значение \textit{«Место для сбора мусора, помойка.»}, соответствующее первому значению,
предложенному алгоритмом ’Место, куда свозят, сваливают мусор, отходы.’.
Как видно из графика результатов алгоритма, оно почти не используется в досоветский период,
но становится главным
с 40\% использования в советский период и доминирует в постсоветский с около 70\%.
Эти данные совпадают с тем, что говорится в книге, где утверждается 87\% использования
значения \textit{«Помойка.»} в 1998-1997 годы, 32\% для 1925-1949 годов.

Уменьшается же судя по графику преимущественно значение 2 (’Столкновение, драка.’),
которое падает с 65\% использований в досоветский период до 5\% в постсоветский.
В книге резульаты схожи.
Так, утверждается, что в 1875-1899 году слово имело значение ’Драка.’
в 71\% использований,
а к 1998-1997 значение упало до 12\%.

\noindent % Prevents indentation for this line to align the images at the left margin
\begin{figure}[H]
    \centering % Centers the images
    \includegraphics[width=0.32\textwidth]{img/Book/Свалка 1998-1999}
    \hfill % Fills the space between the images
    \includegraphics[width=0.32\textwidth]{img/Book/Свалка 1875-1799}
    \hfill % Fills the space between the images
    \includegraphics[width=0.32\textwidth]{img/Book/Свалка 1925-1949}
    \caption{Визуализации для слова "Свалка" из книги "Два века в двадцати словах".}
\end{figure}

Таким образом, модель довольно точно отражает реальное изменение значений слова «свалка»
во времени, согласуясь с данными из толкового словаря и историческим исследованием.
Она адекватно выделяет как наиболее широко используемое сегодня значение,
связанное с местом сбора мусора,
так и менее очевидные значения, включая драку и беспорядочное скопление предметов или людей.

%\section*{Знатный}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Знатный}
%	\caption{Изменение значений слова "Знатный"}
%	\label{fig:Знатный}
%\end{figure}
%
%Значения для визуализации слова «Знатный» (Параметры: eps=0.3, min\_samples=40).
%
%\begin{enumerate}
%    \item Принадлежащий к знатному сословию.
%    \item Имеющий высокое общественное положение.
%    \item Очень хороший, превосходный.
%    \item Значительный, значительный.
%\end{enumerate}
%
%\section*{Кануть}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Кануть}
%	\caption{Изменение значений слова "Кануть"}
%	\label{fig:Кануть}
%\end{figure}
%
%Значения для визуализации слова «Кануть» (Параметры: eps=0.8, min\_samples=15).
%
%\begin{enumerate}
%    \item Исчезнуть, пропасть.
%    \item Перестать существовать, исчезнуть.
%    \item Быстро пройти, миновать (о времени, событиях).
%\end{enumerate}
%
%\section*{Классный}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Классный}
%	\caption{Изменение значений слова "Классный"}
%	\label{fig:Классный}
%\end{figure}
%
%Значения для визуализации слова «Классный» (Параметры: eps=0.8, min\_samples=20).
%
%\begin{enumerate}
%    \item Учитель, заведующий классом.
%    \item Предназначенный для занятий в классе.
%    \item Помещение для занятий в школе.
%    \item Относящийся к определённому классу, разряду.
%    \item Обладающий каким-либо качеством, свойством в высшей степени.
%    \item Очень хороший, превосходный.
%\end{enumerate}
%
%\section*{Мама}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Мама}
%	\caption{Изменение значений слова "Мама"}
%	\label{fig:Мама}
%\end{figure}
%
%Значения для визуализации слова «Мама» (Параметры: eps=0.5, min\_samples=100).
%
%\begin{enumerate}
%    \item Употребляется как ласковое или фамильярное обращение к женщине.
%    \item Женщина по отношению к своим детям.
%\end{enumerate}
%
%\section*{Машина}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Машина}
%	\caption{Изменение значений слова "Машина"}
%	\label{fig:Машина}
%\end{figure}
%
%Значения для визуализации слова «Машина» (Параметры: eps=0.9, min\_samples=40).
%
%\begin{enumerate}
%    \item Устройство, механизм.
%    \item Автомобиль, транспортное средство.
%\end{enumerate}
%
%\section*{Молодец}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Молодец (2)}
%	\caption{Изменение значений слова "Молодец"}
%	\label{fig:Молодец}
%\end{figure}
%
%Значения для визуализации слова «Молодец» (Параметры: eps=0.9, min\_samples=40).
%
%\begin{enumerate}
%    \item Молодой человек, юноша.
%    \item Употребляется как похвала.
%    \item Человек, отличающийся силой, здоровьем, удалью и т. п.
%\end{enumerate}
%
%\section*{Пакет}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Пакет}
%	\caption{Изменение значений слова "Пакет"}
%	\label{fig:Пакет}
%\end{figure}
%
%Значения для визуализации слова «Пакет» (Параметры: eps=0.87, min\_samples=7).
%
%\begin{enumerate}
%    \item Свёрток из бумаги, картона и т. п., предназначенный для упаковки чего-либо.
%    \item Документ в такой упаковке.
%    \item Документ, содержащий какую-либо информацию.
%    \item Специальная упаковка для чего-либо.
%    \item Совокупность однородных предметов, сложенных вместе.
%    \item Совокупность чего-либо, объединённая общей идеей, целью.
%    \item Совокупность акций какого-либо предприятия.
%    \item Совокупность программных и аппаратных средств, предназначенных для выполнения определенных задач.
%\end{enumerate}
%
%\section*{Передовой}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Передовой}
%	\caption{Изменение значений слова "Передовой"}
%	\label{fig:Передовой}
%\end{figure}
%
%Значения для визуализации слова «Передовой» (Параметры: eps=0.9, min\_samples=5).
%
%\begin{enumerate}
%    \item Представляющий собой передовую часть войска.
%    \item Находящийся в авангарде общественного развития.
%    \item Находящийся на переднем крае науки, техники и т. п.
%    \item Находящийся, действующий впереди, в непосредственной близости от противника.
%    \item Выдающийся по своим качествам, свойствам.
%    \item Находящийся на более высоком уровне развития по сравнению с другими.
%\end{enumerate}
%
%\section*{Пионер}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Пионер}
%	\caption{Изменение значений слова "Пионер"}
%	\label{fig:Пионер}
%\end{figure}
%
%Значения для визуализации слова «Пионер» (Параметры: eps=0.9, min\_samples=20).
%
%\begin{enumerate}
%    \item Тот, кто первым начал что-либо делать, заниматься чем-либо.
%    \item Участник пионерского движения.
%    \item Первопроходец в каком-либо деле, начинании.
%\end{enumerate}
%
%\section*{Пожалуй}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Пожалуй}
%	\caption{Изменение значений слова "Пожалуй"}
%	\label{fig:Пожалуй}
%\end{figure}
%
%Значения для визуализации слова «Пожалуй» (Параметры: eps=1.1, min\_samples=60).
%
%\begin{enumerate}
%    \item Употребляется при выражении неуверенности предположения, соответствуя по значению словам: возможно, вероятно.
%    \item Употребляется как вежливое обращение к кому-л. с просьбой сделать что-либо.
%    \item Употребляется в значении: возможно, вероятно.
%    \item Употребляется для выражения сомнения, неуверенности в чем-либо.
%\end{enumerate}
%
%\section*{Пока}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Пока}
%	\caption{Изменение значений слова "Пока"}
%	\label{fig:Пока}
%\end{figure}
%
%Значения для визуализации слова «Пока» (Параметры: eps=1.1, min\_samples=10).
%
%\begin{enumerate}
%    \item Употребляется при обозначении временного промежутка, в течение которого совершается действие.
%    \item В настоящее время, в данный момент.
%    \item До тех пор, как.
%    \item Употребляется при прощании.
%\end{enumerate}
%
%\section*{Привет}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Привет}
%	\caption{Изменение значений слова "Привет"}
%	\label{fig:Привет}
%\end{figure}
%
%Значения для визуализации слова «Привет» (Параметры: eps=0.9, min\_samples=15).
%
%\begin{enumerate}
%    \item Приветливое, дружелюбное обращение к кому-либо.
%    \item Употребляется как выражение дружеского расположения, расположения к кому-либо, чего-либо.
%\end{enumerate}
%
%\section*{Пружина}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Пружина}
%	\caption{Изменение значений слова "Пружина"}
%	\label{fig:Пружина}
%\end{figure}
%
%Значения для визуализации слова «Пружина» (Параметры: eps=0.75, min\_samples=25).
%
%\begin{enumerate}
%    \item То, что вызывает, вызывает какие-либо действия, побуждает к каким-либо действиям.
%    \item Механизм, устройство, состоящее из упругого элемента, состоящего из нескольких упругих элементов.
%\end{enumerate}
%
%\section*{Публика}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Публика}
%	\caption{Изменение значений слова "Публика"}
%	\label{fig:Публика}
%\end{figure}
%
%Значения для визуализации слова «Публика» (Параметры: eps=0.9, min\_samples=30).
%
%\begin{enumerate}
%    \item Люди, составляющие общество.
%    \item Люди, находящиеся где-либо, в каком-либо месте.
%    \item Люди, находящиеся где-либо в качестве зрителей, слушателей.
%\end{enumerate}
%
%\section*{Сволочь}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Сволочь}
%	\caption{Изменение значений слова "Сволочь"}
%	\label{fig:Сволочь}
%\end{figure}
%
%Значения для визуализации слова «Сволочь» (Параметры: eps=1.2, min\_samples=20).
%
%\begin{enumerate}
%    \item Употребляется как порицающее или бранное слово.
%    \item Негодные, подлые люди.
%\end{enumerate}
%
%\section*{Стиль}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Стиль}
%	\caption{Изменение значений слова "Стиль"}
%	\label{fig:Стиль}
%\end{figure}
%
%Значения для визуализации слова «Стиль» (Параметры: eps=0.9, min\_samples=20).
%
%\begin{enumerate}
%    \item Совокупность приёмов и средств выражения, свойственных кому-либо или чему-либо, характерный для кого-либо или чего-либо.
%    \item Совокупность художественных приёмов, характерных для какой-либо эпохи, направления в искусстве.
%    \item Система летосчисления, принятая в той или иной стране.
%\end{enumerate}
%
%\section*{Тётка}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Тетка}
%	\caption{Изменение значений слова "Тётка"}
%	\label{fig:Тётка}
%\end{figure}
%
%Значения для визуализации слова «Тётка» (Параметры: eps=0.85, min\_samples=40).
%
%\begin{enumerate}
%    \item Сестра отца или матери.
%    \item Взрослая женщина.
%\end{enumerate}
%
%\section*{Тройка}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Тройка}
%	\caption{Изменение значений слова "Тройка"}
%	\label{fig:Тройка}
%\end{figure}
%
%Значения для визуализации слова «Тройка» (Параметры: eps=1.1, min\_samples=10).
%
%\begin{enumerate}
%    \item Группа из трёх человек.
%    \item Кучер, запряженный тройкой лошадей.
%    \item Неудовлетворительная оценка по какому-либо предмету.
%    \item Игральная карта с тремя очками.
%    \item Мужская верхняя одежда из трикотажа.
%    \item Число три.
%    \item Группа из трех объектов.
%\end{enumerate}
%
%\section*{Червяк}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.8\textwidth]{img/Words_Meanings/Червяк}
%	\caption{Изменение значений слова "Червяк"}
%	\label{fig:Червяк}
%\end{figure}
%
%Значения для визуализации слова «Червяк» (Параметры: eps=0.95, min\_samples=15).
%
%\begin{enumerate}
%    \item Мелкое животное (обычно о грызунах).
%    \item Употребляется как порицающее или бранное слово.
%    \item Нечто мелкое, незначительное, не стоящее внимания.
%    \item Мелкий, ничтожный человек.
%    \item Нечто неприятное, вызывающее неприятные ощущения.
%\end{enumerate}

\chapter*{Заключение}

%Таким образом, в результате исследования моделирование определений может быть
%успешно применено для задачи детектирования семантических изменений.
%Несмотря на то, что представленное решение не является лидирующим по качеству,
%оно имеет высокую степень интерпретируемости и предлагает один
%из наиболее наглядных методов визуализации.

XXX

Результаты настоящей работы можно применять для определения степени семантического сдвига лексем,
с наличием визуализации и определений для каждого выявленного значения,
что может быть использовано в лексикологии,
где необходимы актуальные данные для построения новых словарей~\cite{DefinitionGenerationMainArticle}.
Кроме того, модель, позволяющая автоматически генерировать качественные словарные определения,
может быть полезна в таких задачах обработки естественного языка,
как анализ тональности, машинный перевод и разграничение семантической
неоднозначности~\cite{DefinitionModelingReviewAndDatasetAnalysis}.
Кроме того, возможно обучение модели большего размера, что повысит качество генерации.

Ограничениями подхода можно считать необходимость в значительных вычислительных ресурсах.
Несмотря на то, что FRED-T5-1.7B запускается на ЦПУ, запуск на большом количестве вхождений
займет значительное число времени.
Для запуска на ГПУ же необходима видеокарта с 8 ГБ видеопамяти.

Код, использованный во время выполнения настоящей работы, выложен в открытый доступ
на сайте GitHub и может быть воспроизведен.~\cite{WorkGitHub}

\printbibliography
\appendix
\chapter{Обучение модели}

\begin{longtable}{ll}
\caption{LoRa параметры} \\
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
r & 32 \\
lora\_alpha & 64 \\
lora\_dropout & 0.1 \\
\hline
\end{longtable}

\begin{longtable}{ll}
\caption{Trainer параметры} \\
\hline
\textbf{Параметр} & \textbf{Значение} \\
\hline
learning\_rate & 1e-3 \\
lr\_scheduler\_type & constant \\
batch\_size & 16 \\
gradient\_checkpointing & true \\
gradient\_accumulation\_steps & 1 \\
weight\_decay & 0.01 \\
optimizer & adafactor \\
num\_train\_epochs & 4 \\
\hline
\end{longtable}

\begin{FIGURE}[h]{Лосс при обучении модели \label{fig:loss-plot-epoch}}
\includegraphics[width=1.0\textwidth]{img/loss-plot-epoch}
\end{FIGURE}

\end{document}
